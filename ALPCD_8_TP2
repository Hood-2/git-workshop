import requests
import json
import re
from bs4 import BeautifulSoup

API_KEY = 'b6e101bc38980bc8302263c0e78ff3f3'
headers_api = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
headers_amb = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0"
    }

def obter_dados(page):
    url = f'https://api.itjobs.pt/job/list.json?api_key={API_KEY}&page={page}'

    response = requests.get(url, headers=headers_api)

    if response.status_code == 200:
        return response.json()
    else:
        print(f"Erro na requisição: {response.status_code}")
        return None
    
def benefits_soup(company):
    try :
        url = f'https://www.ambitionbox.com/benefits/{company}-benefits'
        response = requests.get(url, headers=headers_amb)

        if response.status_code == 200:

            return BeautifulSoup(response.text,'html.parser')
        else:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None
    
def obter_soup(company):
    try :
        url = f'https://www.ambitionbox.com/overview/{company}-overview'
        response = requests.get(url, headers=headers_amb)
        if response.status_code == 200:

            return BeautifulSoup(response.text,'lxml')
        else:
            print(f"Erro a obter website: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website: {response.status_code}")
            return None

@app.command()
def salary(jobid: str):
    page = 1
    while True:
        dados = obter_dados(page)
        if not dados or "results" not in dados:
            break

        job_found = False  # Flag to check if the job ID is found

        for job in dados["results"]:
            if job["id"] == int(jobid):
                job_found = True  # Job ID found
                # Check if the wage is available
                if job["wage"]:
                    print(f"Salary for job ID {jobid}: {job['wage']}")
                    return job["wage"]
                else:
                    # If the wage is not provided, search for salary in the body
                    body_text = job["body"]
                    # Regex to find salary in body text (supports different formats)
                    salary_match = re.search(r'\b(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(euros?|usd|dollars?|€|\$|eur|usd|dol)\b', body_text, re.IGNORECASE)
                    if salary_match:
                        salary_value = salary_match.group(0)
                        print(f"Salary found in description for job ID {jobid}: {salary_value}")
                        return salary_value
                print("Salary not found in job ID:", jobid)
                return "Salary not found"

        page += 1

    print("Job ID not found")
    raise SystemExit 


@app.command(help="List job titles that require specific skills within a given date range.")
def skills(
    skills: str, 
    start_date: str,
    end_date: str,
    output_format: Annotated[str, typer.Argument(help="Specify 'csv' to create a CSV file.")] = None):
    
    """Example: >python file_name.py skills "Python, SQL" 2024-01-01 2024-12-31"""
    
    skill_list = [skill.strip().lower() for skill in skills.split(",")]
    job_details = [] 
    page = 1

    date_istamp = time.mktime(time.strptime(start_date, '%Y-%m-%d'))
    date_fstamp = time.mktime(time.strptime(end_date, '%Y-%m-%d'))

    while True:
        job_data = obter_dados(page)
        if not job_data or "results" not in job_data:
            break
        
        for job in job_data["results"]:
            job_title = job.get("title", "")
            job_body = job.get("body", "").lower()
            job_date = job.get("publishedAt", "")

            job_datestamp = time.mktime(time.strptime(job_date.split(" ")[0], '%Y-%m-%d'))
            if not (date_istamp <= job_datestamp <= date_fstamp):
                continue

            if all(skill in job_body for skill in skill_list):
                job_details.append({
                    "titulo": job_title,
                    "empresa": job.get("company", {}).get("name", "N/A"),
                    "descrição": job.get("body", "N/A"),
                    "data de publicação": job.get("publishedAt", "N/A"),
                    "salário": job.get("wage", "N/A"),
                    "localização": ", ".join([loc["name"] for loc in job.get("locations", [])]) if job.get("locations") else "N/A"
                })

        page += 1

    typer.echo(json.dumps(job_details, indent=4))

    if output_format == 'csv':
        export_csv({"results": job_details}, filename='recent_job_details.csv')
        print("CSV file 'skills.csv' created successfully.")

    with open(f"jobs_{skills}.json", "w", encoding="utf-8") as file:
        json.dump({"results": job_details}, file, ensure_ascii=False, indent=4)
    print(f"The JSON file 'jobs_{skills}.json' was created successfully!")

def get(job_id):
    page = 1
    while True:
        dados = obter_dados(page)
        if not dados or "results" not in dados:
            break
        for job in dados["results"]:
            if job["id"] == job_id:
                # Retornar o nome da empresa
                company = job["company"]["name"]
                print(company)
                company_ = re.sub("\s+", "-", company.lower())
                soup = obter_soup(company_)
                if soup is None:
                    return None
                tag = soup.find('div', class_='css-146c3p1 text-primary-text font-pn-400 text-sm !w-[calc(100%-70px)] line-clamp-1')
                exp = r'is rated (\d\.\d) out of 5 stars'
                if tag is not None :
                    rating = re.findall(exp, tag.text , flags=re.IGNORECASE)
                soup_ben = benefits_soup(company_)
                if soup_ben is None:
                    return None
                ben = soup_ben.find_all("div", class_="css-146c3p1 text-primary-text font-pn-600 text-[16px] leading-[24px] mb-1")
                ben_desc = soup_ben.find_all("div", class_="css-146c3p1 text-neutral-500 font-pn-400 text-[16px] leading-[24px]")
                ben_review = soup_ben.find_all("div" ,class_="css-146c3p1 text-[12px] font-pn-400 leading[1.33]")
                ben_dic={}
                beneficios = []
                beneficios_desc = []
                n = 0
                if ben:
                    for b in ben:
                     beneficios.append(b.text)
                if ben_review  and not ben:
                    for b in ben_review:
                        beneficios.append(b.text)
                if ben_desc:
                    for b in ben_desc:
                        beneficios_desc.append(b.text)
                        ben_dic[beneficios[n]] = beneficios_desc[n]
                        n += 1
                    beneficios = ben_dic
                dess = soup.find('div', class_='css-146c3p1 font-pn-400 text-sm text-neutral mb-2') #Descriçao resumida
                desb = soup.find('div', class_="text-sm font-pn-400 [&_ul]:list-disc [&_ol]:list-[auto] [&_ul]:ml-5 [&_ol]:ml-5") #Descriçao completa
                if desb is not None:
                    descrição = desb.text
                else:
                    if dess is not None:
                        descrição = dess.text
                job_data = {
                            "id": job_id,
                            "rating": float(rating[0]),  
                            "ambition_box_description": descrição,
                            "ambition_box_benefits": ben_dic
                            }
                print(json.dumps(job_data, indent=4))
                return job_data
        # Passa para a próxima página
        page += 1
    return "ID not found"

def obter_soup_alt(company):
    try :
        url = f'https://pt.indeed.com/cmp/{company}'
        response = requests.get(url, headers=headers)
        if response.status_code == 200:

            return BeautifulSoup(response.text,'lxml')
        else:
            print(f"Erro a obter website: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website: {response.status_code}")
            return None
def benefits_soup_alt(company):
    try :
        url = f"https://pt.indeed.com/cmp/{copmany}/reviews?ftopic=paybenefits"
        response = requests.get(url, headers=headers_amb)
        if response.status_code == 200:

            return BeautifulSoup(response.text,'html.parser')
        else:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None

    
def get_alt(job_id):

    """Example: >python file_name.py get 494386"""

    page = 1
    while True:
        dados = obter_dados(page)
        if not dados or "results" not in dados:
            break
        for job in dados["results"]:
            if job["id"] == job_id:
                # Retornar o nome da empresa
                company = job["company"]["name"]
                company_ = re.sub(r"\s+", "-", company)
                soup = obter_soup_alt(company_)
                rating = soup.find("span", class_="css-4f2law e1wnkr790").text
                texto_div = soup.find('div', class_="css-9146s eu4oa1w0", attrs={"data-testid": "more-text"})
                if texto_div:
                # Buscar todos os parágrafos (<p>) dentro do div
                    text = texto_div.find_all('p')
                descriçao = []
            for p in text:
                descriçao.append(p.text)

            soup_b = benefits_soup_alt(company_)
            vantagens_div = soup_b.find_all('div', class_="css-zaij7l e1tiznh50", string="Vantagens")
            beneficios = []
            if vantagens_div:
                #Procurar diretamente o elemento <span> dentro de vantagens_div
                span_beneficio = vantagens_div.find('span', class_="css-15r9gu1 eu4oa1w0")
            if span_beneficio:
                for b in span_beneficio:
                    beneficios.append(b.text)
            job_data = {
                            "id": job_id,
                            "rating": float(rating[0]),  
                            "ambition_box_description": descriçao,
                            "ambition_box_benefits": beneficios
                            }
            print(json.dumps(job_data, indent=4))
            return job_data
        # Passa para a próxima página
        page += 1
    return "ID not found"
