import requests
import json
import re
from bs4 import BeautifulSoup
import typer
import requests
import csv
from typing import Annotated
import time

app = typer.Typer()

api_key = "b6e101bc38980bc8302263c0e78ff3f3"
url = "https://api.itjobs.pt/job/list.json"
response = requests.get(url, headers={"User-Agent": "Mozilla/5.0"})
dados = response.json()
headers_api = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
}
headers_amb = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "Sec-Fetch-Dest": "document",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-User": "?1",
    "Cache-Control": "max-age=0"
    }

def obter_dados(page):
    url = f'https://api.itjobs.pt/job/list.json?api_key={API_KEY}&page={page}'

    response = requests.get(url, headers=headers_api)

    if response.status_code == 200:
        return response.json()
    else:
        print(f"Erro na requisição: {response.status_code}")
        return None

def export_csv(data, filename="recent_jobs.csv"):
    if isinstance(data, list):

        if not data:
            print("No data to export.")
            return
        
        fieldnames = data[0].keys()
    elif isinstance(data, dict) and "results" in data:

        data = data["results"]
        if not data:
            print("No data to export.")
            return
        
        fieldnames = data[0].keys()
    else:
        print("Invalid data format for CSV export.")
        return

    with open(filename, "w", newline='', encoding="utf-8") as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()
        
        if isinstance(data, list):
            writer.writerows(data)
        else:
            for job in data:
                writer.writerow(job)
    
    print(f"The CSV file '{filename}' was created successfully!")

def benefits_soup(company):
    try :
        url = f'https://www.ambitionbox.com/benefits/{company}-benefits'
        response = requests.get(url, headers=headers_amb)

        if response.status_code == 200:

            return BeautifulSoup(response.text,'html.parser')
        else:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None
    
def obter_soup(company):
    try :
        url = f'https://www.ambitionbox.com/overview/{company}-overview'
        response = requests.get(url, headers=headers_amb)
        if response.status_code == 200:

            return BeautifulSoup(response.text,'lxml')
        else:
            print(f"Erro a obter website: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website: {response.status_code}")
            return None

@app.command(help="List the N most recent jobs published on itjobs.pt.")
def top(n: int, 
        output_format: Annotated[str, typer.Argument(help="Specify 'csv' to create a CSV file.")] = None):

    """Example: >python file_name.py top 5"""

    new_url = f"{url}?api_key={api_key}&limit={n}"
    response = requests.get(new_url, headers={"User-Agent": "Mozilla/5.0"})
    dados = response.json()

    print(f"\n{n} most recent job titles:")
    job_details = [
        {
            "titulo": job["title"],
            "empresa": job.get("company", {}).get("name", "N/A"),
            "descrição": job.get("body", "N/A"),
            "data de publicação": job.get("publishedAt", "N/A"),
            "salário": job.get("wage", "N/A"),
            "localização": ", ".join([loc["name"] for loc in job.get("locations", [])]) if job.get("locations") else "N/A"
        }
        for job in dados.get("results", [])
    ]

    for job in job_details:
        print(job["titulo"])

    if output_format == 'csv':
        export_csv({"results": job_details}, filename='recent_job_details.csv')
        print("CSV file 'recent_job_details.csv' created successfully.")

    with open(f"{n}_recent_job_details.json", "w", encoding="utf-8") as file:
        json.dump(dados, file, indent=4)
    print(f"The JSON file '{n}_recent_job_details.json' was created successfully!")


@app.command(help="Search for job postings based on location, company, and number of jobs.")
def search(
    location: str, company: str, num_jobs: int, 
    output_format: Annotated[str, typer.Argument(help="Specify 'csv' to create a CSV file.")] = None):

    """Example: >python file.py search "Lisboa" "KWAN" 5"""
    job_details = []
    page = 1
    location = location.lower()
    company = company.lower()

    while len(job_details) < num_jobs:
        job_data = obter_dados(page)  
        if not job_data or "results" not in job_data:
            break

        for job in job_data["results"]:
            job_location = [loc["name"].lower() for loc in job.get("locations", [])]
            job_company = job.get("company", {}).get("name", "").lower()
            job_types = [t["name"].lower() for t in job.get("types", [])]

            if location in job_location and company in job_company and "full-time" in job_types:
                job_details.append({
                    "titulo": job["title"],
                    "empresa": job.get("company", {}).get("name", "N/A"),
                    "descrição": job.get("body", "N/A"),
                    "data de publicação": job.get("publishedAt", "N/A"),
                    "salário": job.get("wage", "N/A"),
                    "localização": ", ".join([loc["name"] for loc in job.get("locations", [])]) if job.get("locations") else "N/A"
                })
                if len(job_details) >= num_jobs:
                    break
        page += 1

    for job in job_details:
        print(job["titulo"])

    if output_format == 'csv':
        export_csv({"results": job_details}, filename='recent_job_details.csv')
        print("CSV file 'recent_job_details.csv' created successfully.")
    
    with open(f"jobs_{location}_{company}.json", "w", encoding="utf-8") as file:
        json.dump({"results": job_details}, file, ensure_ascii=False, indent=4)
    print(f"The JSON file 'jobs_{location}_{company}.json' was created successfully!")


@app.command(help="Gives information about a wage of a certain job.")
def salary(jobid: str):
    page = 1
    while True:
        dados = obter_dados(page)
        if not dados or "results" not in dados:
            break

        job_found = False  # Flag to check if the job ID is found

        for job in dados["results"]:
            if job["id"] == int(jobid):
                job_found = True  # Job ID found
                # Check if the wage is available
                if job["wage"]:
                    print(f"Salary for job ID {jobid}: {job['wage']}")
                    return job["wage"]
                else:
                    # If the wage is not provided, search for salary in the body
                    body_text = job["body"]
                    # Regex to find salary in body text (supports different formats)
                    salary_match = re.search(r'\b(\d{1,3}(?:,\d{3})*(?:\.\d{2})?)\s*(euros?|usd|dollars?|€|\$|eur|usd|dol)\b', body_text, re.IGNORECASE)
                    if salary_match:
                        salary_value = salary_match.group(0)
                        print(f"Salary found in description for job ID {jobid}: {salary_value}")
                        return salary_value
                print("Salary not found in job ID:", jobid)
                return "Salary not found"

        page += 1

    print("Job ID not found")
    raise SystemExit 


@app.command(help="List job titles that require specific skills within a given date range.")
def skills(
    skills: str, 
    start_date: str,
    end_date: str,
    output_format: Annotated[str, typer.Argument(help="Specify 'csv' to create a CSV file.")] = None):
    
    """Example: >python file_name.py skills "Python, SQL" 2024-01-01 2024-12-31"""
    
    skill_list = [skill.strip().lower() for skill in skills.split(",")]
    job_details = [] 
    page = 1

    date_istamp = time.mktime(time.strptime(start_date, '%Y-%m-%d'))
    date_fstamp = time.mktime(time.strptime(end_date, '%Y-%m-%d'))

    while True:
        job_data = obter_dados(page)
        if not job_data or "results" not in job_data:
            break
        
        for job in job_data["results"]:
            job_title = job.get("title", "")
            job_body = job.get("body", "").lower()
            job_date = job.get("publishedAt", "")

            job_datestamp = time.mktime(time.strptime(job_date.split(" ")[0], '%Y-%m-%d'))
            if not (date_istamp <= job_datestamp <= date_fstamp):
                continue

            if all(skill in job_body for skill in skill_list):
                job_details.append({
                    "titulo": job_title,
                    "empresa": job.get("company", {}).get("name", "N/A"),
                    "descrição": job.get("body", "N/A"),
                    "data de publicação": job.get("publishedAt", "N/A"),
                    "salário": job.get("wage", "N/A"),
                    "localização": ", ".join([loc["name"] for loc in job.get("locations", [])]) if job.get("locations") else "N/A"
                })

        page += 1

    typer.echo(json.dumps(job_details, indent=4))

    if output_format == 'csv':
        export_csv({"results": job_details}, filename='recent_job_details.csv')
        print("CSV file 'skills.csv' created successfully.")

    with open(f"jobs_{skills}.json", "w", encoding="utf-8") as file:
        json.dump({"results": job_details}, file, ensure_ascii=False, indent=4)
    print(f"The JSON file 'jobs_{skills}.json' was created successfully!")

@app.command(help="Lists information about a specific jobID.")
def get(job_id):
    """Example: >python file_name.py get 494111"""
    page = 1
    while True:
        dados = obter_dados(page)
        if not dados or "results" not in dados:
            break
        for job in dados["results"]:
            if job["id"] == job_id:
                # Retornar o nome da empresa
                company = job["company"]["name"]
                print(company)
                company_ = re.sub("\s+", "-", company.lower())
                soup = obter_soup(company_)
                if soup is None:
                    return None
                tag = soup.find('div', class_='css-146c3p1 text-primary-text font-pn-400 text-sm !w-[calc(100%-70px)] line-clamp-1')
                exp = r'is rated (\d\.\d) out of 5 stars'
                if tag is not None :
                    rating = re.findall(exp, tag.text , flags=re.IGNORECASE)
                soup_ben = benefits_soup(company_)
                if soup_ben is None:
                    return None
                ben = soup_ben.find_all("div", class_="css-146c3p1 text-primary-text font-pn-600 text-[16px] leading-[24px] mb-1")
                ben_desc = soup_ben.find_all("div", class_="css-146c3p1 text-neutral-500 font-pn-400 text-[16px] leading-[24px]")
                ben_review = soup_ben.find_all("div" ,class_="css-146c3p1 text-[12px] font-pn-400 leading[1.33]")
                ben_dic={}
                beneficios = []
                beneficios_desc = []
                n = 0
                if ben:
                    for b in ben:
                     beneficios.append(b.text)
                if ben_review  and not ben:
                    for b in ben_review:
                        beneficios.append(b.text)
                if ben_desc:
                    for b in ben_desc:
                        beneficios_desc.append(b.text)
                        ben_dic[beneficios[n]] = beneficios_desc[n]
                        n += 1
                    beneficios = ben_dic
                dess = soup.find('div', class_='css-146c3p1 font-pn-400 text-sm text-neutral mb-2') #Descriçao resumida
                desb = soup.find('div', class_="text-sm font-pn-400 [&_ul]:list-disc [&_ol]:list-[auto] [&_ul]:ml-5 [&_ol]:ml-5") #Descriçao completa
                if desb is not None:
                    descrição = desb.text
                else:
                    if dess is not None:
                        descrição = dess.text
                job_data = {
                            "id": job_id,
                            "rating": float(rating[0]),  
                            "ambition_box_description": descrição,
                            "ambition_box_benefits": ben_dic
                            }
                print(json.dumps(job_data, indent=4))
                return job_data
        # Passa para a próxima página
        page += 1
    return "ID not found"

    if job_data:
        export_csv({"results": [job_data]}, filename=f'jobid_{job_id}_information.csv')
        print(f"CSV file 'jobid_{job_id}_information.csv' created successfully.")
    else:
        print(f"Job ID {job_id} not found.")

@app.command(help="List the number of job openings by title and location on itjobs.pt.")
def statistics():
    """Example: >python file_name.py statistics"""

    job_counts = {}
    page = 1
    has_more_results = True

    while has_more_results:
        dados = obter_dados(page)
        if dados is None or not dados.get("results"):
            has_more_results = False
            continue

        for job in dados.get("results", []):
            title = job["title"]
            locations = ", ".join([loc["name"] for loc in job.get("locations", [])]) if job.get("locations") else "N/A"
            key = (title, locations)

            if key in job_counts:
                job_counts[key] += 1
            else:
                job_counts[key] = 1

        page += 1

    aggregated_data = [
        {"titulo": title, "localização": location, "número de vagas": count}
        for (title, location), count in job_counts.items()
    ]

    if aggregated_data:
        #fieldnames = ["titulo", "localização", "número de vagas"]
        export_csv({"results": aggregated_data}, filename='job_openings.csv')
        print(f"CSV file 'job_openings.csv' created successfully.")
    else:
        print("No data to export.")   

def obter_soup_alt(company):
    try :
        url = f'https://pt.indeed.com/cmp/{company}'
        response = requests.get(url, headers=headers)
        if response.status_code == 200:

            return BeautifulSoup(response.text,'lxml')
        else:
            print(f"Erro a obter website: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website: {response.status_code}")
            return None
def benefits_soup_alt(company):
    try :
        url = f"https://pt.indeed.com/cmp/{copmany}/reviews?ftopic=paybenefits"
        response = requests.get(url, headers=headers_amb)
        if response.status_code == 200:

            return BeautifulSoup(response.text,'html.parser')
        else:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None
    except requests.exceptions.RequestException as e:
            print(f"Erro a obter website-benefits: {response.status_code}")
            return None

@app.command(help="Lists information about a specific jobID from an alternative website.")       
def get_alt(job_id):

    """Example: >python file_name.py get 494386"""

    page = 1
    while True:
        dados = obter_dados(page)
        if not dados or "results" not in dados:
            break
        for job in dados["results"]:
            if job["id"] == job_id:
                # Retornar o nome da empresa
                company = job["company"]["name"]
                company_ = re.sub(r"\s+", "-", company)
                soup = obter_soup_alt(company_)
                rating = soup.find("span", class_="css-4f2law e1wnkr790").text
                texto_div = soup.find('div', class_="css-9146s eu4oa1w0", attrs={"data-testid": "more-text"})
                if texto_div:
                # Buscar todos os parágrafos (<p>) dentro do div
                    text = texto_div.find_all('p')
                descriçao = []
            for p in text:
                descriçao.append(p.text)

            soup_b = benefits_soup_alt(company_)
            vantagens_div = soup_b.find_all('div', class_="css-zaij7l e1tiznh50", string="Vantagens")
            beneficios = []
            if vantagens_div:
                #Procurar diretamente o elemento <span> dentro de vantagens_div
                span_beneficio = vantagens_div.find('span', class_="css-15r9gu1 eu4oa1w0")
            if span_beneficio:
                for b in span_beneficio:
                    beneficios.append(b.text)
            job_data = {
                            "id": job_id,
                            "rating": float(rating[0]),  
                            "ambition_box_description": descriçao,
                            "ambition_box_benefits": beneficios
                            }
            print(json.dumps(job_data, indent=4))
            return job_data
        # Passa para a próxima página
        page += 1
    return "ID not found"


@app.command()
def list_skills(job_title: str):

    job_title_formatted = job_title.replace(" ", "%20")
    url = f"https://www.ambitionbox.com/jobs/search?tag={job_title_formatted}"
    response = requests.get(url, headers=headers_amb)

    if response.status_code != 200:
        typer.echo("Erro no acesso à página.")

        return

    soup = BeautifulSoup(response.text, 'html.parser')
    skills_section = soup.find('p', class_='body-medium sub-title', string='Key skills for the job')
    if not skills_section:
        typer.echo("Skills não encontradas.")

        return

    skills_div = skills_section.find_next_sibling('div', class_='show-flex chips-cont')
    if not skills_div:
        typer.echo("Skills não encontradas.")

        return

    skills = [skill.get_text(strip=True) for skill in skills_div.find_all('a', class_='body-medium chip')]
    skill_counts = Counter(skills)
    top_skills = skill_counts.most_common(10)

    output = [{"skill": skill, "count": count} for skill, count in top_skills]
    print(json.dumps(output, indent=4))


if __name__ == "__main__":
    app()
